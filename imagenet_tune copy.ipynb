{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/content_understanding/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn import calibration\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.tree import DecisionTreeRegressor as DT\n",
    "from sklearn.isotonic import IsotonicRegression as IR\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ViT-B-16'\n",
    "pretrained_dset = 'laion400m_e31'\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(model_name,\n",
    "    pretrained=pretrained_dset,\n",
    "    device=device)\n",
    "tokenizer = open_clip.get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_test = datasets.ImageFolder(f'/home/ubuntu/data/Imagenet/ILSVRC/Data/CLS-LOC/train/', transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:18<00:00, 12.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_template = 'a photo of a {}.'\n",
    "np.random.seed(0)\n",
    "\n",
    "subset_in_classes = []\n",
    "for c in tqdm(range(len(imagenet_test.classes))):\n",
    "    appending = np.random.choice(np.where(np.array(imagenet_test.targets) == c)[0], 50, replace=False)\n",
    "    subset_in_classes.extend(appending.tolist())\n",
    "subset = torch.utils.data.Subset(imagenet_test, subset_in_classes)\n",
    "subset.classes = imagenet_test.classes\n",
    "imagenet_test = subset\n",
    "len(imagenet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_consider = [\n",
    "    ('ViT-B-16' , 'laion2b_s34b_b88k'),\n",
    "    ('ViT-L-14' , 'laion2b_s32b_b82k'),\n",
    "    ('ViT-B-32' , 'laion2b_s34b_b79k'),\n",
    "\n",
    "    ('ViT-B-16' , 'openai'),\n",
    "    ('ViT-L-14' , 'openai'),\n",
    "    ('ViT-B-32' , 'openai'),\n",
    "\n",
    "    ('ViT-B-16' , 'laion400m_e31'),\n",
    "    ('ViT-L-14' , 'laion400m_e31'),\n",
    "    ('ViT-B-32' , 'laion400m_e31'),\n",
    "\n",
    "    ('RN50', 'openai'),\n",
    "    ('RN50', 'yfcc15m'),\n",
    "    ('RN50', 'cc12m'),\n",
    "\n",
    "    ('ViT-H-14', 'laion2b_s32b_b79k')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [29:06<00:00, 134.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ViT-B-16;laion2b_s34b_b88k': 1.3878337144851685,\n",
       " 'ViT-L-14;laion2b_s32b_b82k': 1.4129570722579956,\n",
       " 'ViT-B-32;laion2b_s34b_b79k': 1.4092495441436768,\n",
       " 'ViT-B-16;openai': 0.9825258851051331,\n",
       " 'ViT-L-14;openai': 1.0273551940917969,\n",
       " 'ViT-B-32;openai': 0.9789162874221802,\n",
       " 'ViT-B-16;laion400m_e31': 1.6131701469421387,\n",
       " 'ViT-L-14;laion400m_e31': 1.7537566423416138,\n",
       " 'ViT-B-32;laion400m_e31': 1.37242591381073,\n",
       " 'RN50;openai': 0.9742085933685303,\n",
       " 'RN50;yfcc15m': 2.5651497840881348,\n",
       " 'RN50;cc12m': 2.662893295288086,\n",
       " 'ViT-H-14;laion2b_s32b_b79k': 1.5119352340698242}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imagenet_mapping = map_imagenet_to_readable_label()\n",
    "#imagenet_test.classes = [imagenet_mapping[x] for x in imagenet_test.classes ]\n",
    "\n",
    "all_temps = {}\n",
    "for model_name, pretrained_dset in tqdm(models_to_consider):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(model_name,\n",
    "        pretrained=pretrained_dset,\n",
    "        device=device)\n",
    "    tokenizer = open_clip.get_tokenizer(model_name)\n",
    "    imagenet_test.dataset.transform = preprocess\n",
    "    \n",
    "    image_features, actual = get_image_features(model, imagenet_test, batch_size=128,\n",
    "        device = device)\n",
    "\n",
    "    actual = torch.IntTensor(actual).to(device).long()\n",
    "\n",
    "    text = tokenizer([text_template.replace('{}',x) for x in imagenet_test.classes])\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text.to(device))\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "    text_probs = (100.0 * image_features @ text_features.T)\n",
    "\n",
    "    ## Setup LBGFS\n",
    "    temperature = nn.Parameter((torch.ones(1)).to(device))\n",
    "    args = {'temperature': temperature}\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Removing strong_wolfe line search results in jump after 50 epochs\n",
    "    optimizer = optim.LBFGS([temperature], lr=0.001, max_iter=1000, line_search_fn='strong_wolfe')\n",
    "\n",
    "    temps = []\n",
    "    losses = []\n",
    "    def _eval():\n",
    "        loss = criterion(T_scaling(text_probs, args), actual)\n",
    "        loss.backward()\n",
    "        temps.append(temperature.item())\n",
    "        losses.append(loss)\n",
    "        return loss\n",
    "    optimizer.step(_eval)\n",
    "    all_temps[model_name+';' + pretrained_dset] = temperature.item()\n",
    "all_temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [06:40<00:00, 30.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [06:38<00:00, 30.66s/it]\n",
      "100%|██████████| 13/13 [14:19<00:00, 66.15s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_eces = {}\n",
    "dataset_accs = {}\n",
    "for dataset_name in ['CIFAR100', 'CIFAR10', 'Food101', 'SUN397']:\n",
    "    dset,_ = get_test_set(dataset_name, None)\n",
    "    model_eces = {}\n",
    "    model_accs = {}\n",
    "    for model_legend, temp in tqdm(all_temps.items()):\n",
    "        model_name, pretrained_dset = model_legend.split(';')\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(model_name,\n",
    "            pretrained=pretrained_dset,\n",
    "            device=device)\n",
    "        tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        dset.transform = preprocess\n",
    "        _ , templates = get_openai_prompts(dataset_name)\n",
    "\n",
    "        template_eces = []\n",
    "        template_accs = []\n",
    "        image_features, actual = get_image_features(\n",
    "            model,  dset,  batch_size=batch_size, device=device\n",
    "        )\n",
    "        for text_template in templates:\n",
    "            #predictions, actual, probs = get_preds(model, tokenizer, dset, \n",
    "            #    text_template=text_template, temp_scaling=temp, device=device)\n",
    "\n",
    "            predictions, probs = get_preds_from_img_features(model, tokenizer, dset, image_features, text_template=text_template, temp_scaling=temp,\n",
    "                device = device)\n",
    "\n",
    "            ECE, _, acc = get_metrics(predictions, actual, probs)\n",
    "            template_eces.append(ECE)\n",
    "            template_accs.append(acc)\n",
    "        \n",
    "        model_eces[model_legend] = template_eces\n",
    "        model_accs[model_legend] = template_accs\n",
    "    dataset_eces[dataset_name] = model_eces\n",
    "    dataset_accs[dataset_name] = model_eces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_supervised_fromimagefeatures(model, tokenizer, text_template, image_features, actual, device):\n",
    "    actual = torch.IntTensor(actual).to(device).long()\n",
    "\n",
    "    text = tokenizer([text_template.replace('{}',x) for x in dset.classes])\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        text_features = model.encode_text(text.to(device))\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "    text_probs = (100.0 * image_features @ text_features.T)\n",
    "\n",
    "    ## Setup LBGFS\n",
    "    temperature = nn.Parameter((torch.ones(1)).to(device))\n",
    "    args = {'temperature': temperature}\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Removing strong_wolfe line search results in jump after 50 epochs\n",
    "    optimizer = optim.LBFGS([temperature], lr=0.001, max_iter=1000, line_search_fn='strong_wolfe')\n",
    "\n",
    "    temps = []\n",
    "    losses = []\n",
    "    def _eval():\n",
    "        loss = criterion(T_scaling(text_probs, args), actual)\n",
    "        loss.backward()\n",
    "        temps.append(temperature.item())\n",
    "        losses.append(loss)\n",
    "        return loss\n",
    "    optimizer.step(_eval)\n",
    "    return temperature.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [09:20<00:00, 43.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ViT-B-16;laion2b_s34b_b88k  ViT-L-14;laion2b_s32b_b82k  \\\n",
      "0                    0.052960                    0.044266   \n",
      "1                    0.015577                    0.027554   \n",
      "2                    0.046688                    0.040429   \n",
      "3                    0.034539                    0.067685   \n",
      "4                    0.033247                    0.079353   \n",
      "5                    0.025430                    0.067133   \n",
      "6                    0.048645                    0.044957   \n",
      "7                    0.025491                    0.067092   \n",
      "\n",
      "   ViT-B-32;laion2b_s34b_b79k  ViT-B-16;openai  ViT-L-14;openai  \\\n",
      "0                    0.050882         0.036334         0.026013   \n",
      "1                    0.023034         0.021782         0.019164   \n",
      "2                    0.034367         0.032281         0.023873   \n",
      "3                    0.042729         0.053331         0.018901   \n",
      "4                    0.056648         0.036224         0.043715   \n",
      "5                    0.032343         0.021942         0.028195   \n",
      "6                    0.047643         0.030547         0.021028   \n",
      "7                    0.033056         0.034285         0.039070   \n",
      "\n",
      "   ViT-B-32;openai  ViT-B-16;laion400m_e31  ViT-L-14;laion400m_e31  \\\n",
      "0         0.058021                0.052931                0.054784   \n",
      "1         0.032883                0.030608                0.020693   \n",
      "2         0.022820                0.068283                0.045057   \n",
      "3         0.045138                0.075536                0.065432   \n",
      "4         0.058913                0.057590                0.053074   \n",
      "5         0.037070                0.049278                0.040016   \n",
      "6         0.036537                0.052487                0.067270   \n",
      "7         0.052728                0.079378                0.069893   \n",
      "\n",
      "   ViT-B-32;laion400m_e31  RN50;openai  RN50;yfcc15m  RN50;cc12m  \\\n",
      "0                0.069151     0.044038      0.054600    0.040337   \n",
      "1                0.037132     0.025307      0.062285    0.018702   \n",
      "2                0.056361     0.029721      0.061457    0.070811   \n",
      "3                0.048939     0.032291      0.065303    0.045386   \n",
      "4                0.059450     0.045273      0.049018    0.048775   \n",
      "5                0.033792     0.027279      0.047657    0.021461   \n",
      "6                0.061638     0.018004      0.036916    0.076166   \n",
      "7                0.056221     0.043703      0.048119    0.047030   \n",
      "\n",
      "   ViT-H-14;laion2b_s32b_b79k  \n",
      "0                    0.044323  \n",
      "1                    0.040272  \n",
      "2                    0.023738  \n",
      "3                    0.034199  \n",
      "4                    0.045388  \n",
      "5                    0.043641  \n",
      "6                    0.037890  \n",
      "7                    0.052537  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [30:43<00:00, 141.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ViT-B-16;laion2b_s34b_b88k  ViT-L-14;laion2b_s32b_b82k  \\\n",
      "0                    0.009581                    0.012874   \n",
      "1                    0.012493                    0.013935   \n",
      "\n",
      "   ViT-B-32;laion2b_s34b_b79k  ViT-B-16;openai  ViT-L-14;openai  \\\n",
      "0                    0.013423         0.038983         0.043229   \n",
      "1                    0.013218         0.024134         0.034394   \n",
      "\n",
      "   ViT-B-32;openai  ViT-B-16;laion400m_e31  ViT-L-14;laion400m_e31  \\\n",
      "0         0.036582                0.008826                0.015309   \n",
      "1         0.032753                0.011028                0.013897   \n",
      "\n",
      "   ViT-B-32;laion400m_e31  RN50;openai  RN50;yfcc15m  RN50;cc12m  \\\n",
      "0                0.011505     0.028224      0.019826    0.020423   \n",
      "1                0.013701     0.025124      0.021942    0.028202   \n",
      "\n",
      "   ViT-H-14;laion2b_s32b_b79k  \n",
      "0                    0.012386  \n",
      "1                    0.017874  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#dataset_eces_supervised = {}\n",
    "#dataset_temps_supervised = {}\n",
    "for dataset_name in ['DTD', 'SUN397',]:#['CIFAR100', 'CIFAR10', 'Food101', 'SUN397']:\n",
    "    #if dataset_name in dataset_eces_supervised:\n",
    "    #    continue\n",
    "    dset,_ = get_test_set(dataset_name, None)\n",
    "    classes , templates = get_openai_prompts(dataset_name)\n",
    "    val_dset = get_val_set(dataset_name, classes, None)\n",
    "\n",
    "    model_eces = {}\n",
    "    model_temps = {}\n",
    "    for model_legend, _ in tqdm(all_temps.items()):\n",
    "        model_name, pretrained_dset = model_legend.split(';')\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(model_name,\n",
    "            pretrained=pretrained_dset,\n",
    "            device=device)\n",
    "        tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        if isinstance(dset, torch.utils.data.dataset.Subset):\n",
    "            dset.dataset.transform = preprocess\n",
    "        else:\n",
    "            dset.transform = preprocess\n",
    "\n",
    "        if isinstance(val_dset, torch.utils.data.dataset.Subset):\n",
    "            val_dset.dataset.transform = preprocess\n",
    "        else:\n",
    "            val_dset.transform = preprocess\n",
    "        \n",
    "\n",
    "        template_eces = []\n",
    "        template_temps = []\n",
    "        image_features, actual = get_image_features(\n",
    "            model,  dset,  batch_size=batch_size, device=device\n",
    "        )\n",
    "        val_image_features, actual_val = get_image_features(\n",
    "            model,  val_dset,  batch_size=batch_size, device=device\n",
    "        )\n",
    "        for text_template in templates:\n",
    "            #predictions, actual, probs = get_preds(model, tokenizer, dset, \n",
    "            #    text_template=text_template, temp_scaling=temp, device=device)\n",
    "            sup_temp = run_supervised_fromimagefeatures(model, tokenizer, text_template, val_image_features, actual_val, device)\n",
    "            predictions, probs = get_preds_from_img_features(model, tokenizer, dset, image_features, text_template=text_template, temp_scaling=sup_temp,\n",
    "                device = device)\n",
    "\n",
    "            ECE, _, acc = get_metrics(predictions, actual, probs)\n",
    "            template_eces.append(ECE)\n",
    "            template_temps.append(sup_temp)\n",
    "        \n",
    "        model_eces[model_legend] = template_eces\n",
    "        model_temps[model_legend] = template_temps\n",
    "    dataset_eces_supervised[dataset_name] = model_eces\n",
    "    print(pd.DataFrame(model_eces))\n",
    "    dataset_temps_supervised[dataset_name] = model_temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ViT-B-16;laion2b_s34b_b88k</th>\n",
       "      <th>ViT-L-14;laion2b_s32b_b82k</th>\n",
       "      <th>ViT-B-32;laion2b_s34b_b79k</th>\n",
       "      <th>ViT-B-16;openai</th>\n",
       "      <th>ViT-L-14;openai</th>\n",
       "      <th>ViT-B-32;openai</th>\n",
       "      <th>ViT-B-16;laion400m_e31</th>\n",
       "      <th>ViT-L-14;laion400m_e31</th>\n",
       "      <th>ViT-B-32;laion400m_e31</th>\n",
       "      <th>RN50;openai</th>\n",
       "      <th>RN50;yfcc15m</th>\n",
       "      <th>RN50;cc12m</th>\n",
       "      <th>ViT-H-14;laion2b_s32b_b79k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009581</td>\n",
       "      <td>0.012874</td>\n",
       "      <td>0.013423</td>\n",
       "      <td>0.038983</td>\n",
       "      <td>0.043229</td>\n",
       "      <td>0.036582</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>0.015309</td>\n",
       "      <td>0.011505</td>\n",
       "      <td>0.028224</td>\n",
       "      <td>0.019826</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>0.012386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012493</td>\n",
       "      <td>0.013935</td>\n",
       "      <td>0.013218</td>\n",
       "      <td>0.024134</td>\n",
       "      <td>0.034394</td>\n",
       "      <td>0.032753</td>\n",
       "      <td>0.011028</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>0.013701</td>\n",
       "      <td>0.025124</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>0.028202</td>\n",
       "      <td>0.017874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ViT-B-16;laion2b_s34b_b88k  ViT-L-14;laion2b_s32b_b82k  \\\n",
       "0                    0.009581                    0.012874   \n",
       "1                    0.012493                    0.013935   \n",
       "\n",
       "   ViT-B-32;laion2b_s34b_b79k  ViT-B-16;openai  ViT-L-14;openai  \\\n",
       "0                    0.013423         0.038983         0.043229   \n",
       "1                    0.013218         0.024134         0.034394   \n",
       "\n",
       "   ViT-B-32;openai  ViT-B-16;laion400m_e31  ViT-L-14;laion400m_e31  \\\n",
       "0         0.036582                0.008826                0.015309   \n",
       "1         0.032753                0.011028                0.013897   \n",
       "\n",
       "   ViT-B-32;laion400m_e31  RN50;openai  RN50;yfcc15m  RN50;cc12m  \\\n",
       "0                0.011505     0.028224      0.019826    0.020423   \n",
       "1                0.013701     0.025124      0.021942    0.028202   \n",
       "\n",
       "   ViT-H-14;laion2b_s32b_b79k  \n",
       "0                    0.012386  \n",
       "1                    0.017874  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset_eces_supervised['SUN397'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [06:53<00:00, 31.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [06:41<00:00, 30.90s/it]\n",
      "100%|██████████| 13/13 [14:21<00:00, 66.28s/it]\n",
      "  0%|          | 0/13 [02:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/code/clip_miscalibration/imagenet_tune.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/clip_miscalibration/imagenet_tune.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m _ , templates \u001b[39m=\u001b[39m get_openai_prompts(dataset_name)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/clip_miscalibration/imagenet_tune.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m template_eces \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/clip_miscalibration/imagenet_tune.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m image_features, actual \u001b[39m=\u001b[39m get_image_features(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/clip_miscalibration/imagenet_tune.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     model,  dset,  batch_size\u001b[39m=\u001b[39;49mbatch_size, device\u001b[39m=\u001b[39;49mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/clip_miscalibration/imagenet_tune.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/clip_miscalibration/imagenet_tune.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m text_template \u001b[39min\u001b[39;00m templates:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/clip_miscalibration/imagenet_tune.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m#predictions, actual, probs = get_preds(model, tokenizer, dset, \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/clip_miscalibration/imagenet_tune.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m#    text_template=text_template, temp_scaling=temp, device=device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/clip_miscalibration/imagenet_tune.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     predictions, probs \u001b[39m=\u001b[39m get_preds_from_img_features(model, tokenizer, dset, image_features, text_template\u001b[39m=\u001b[39mtext_template, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/clip_miscalibration/imagenet_tune.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39m#temp_scaling=temp,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbenp_8gpu/home/ubuntu/code/clip_miscalibration/imagenet_tune.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         device \u001b[39m=\u001b[39m device)\n",
      "File \u001b[0;32m~/code/clip_miscalibration/util.py:22\u001b[0m, in \u001b[0;36mget_image_features\u001b[0;34m(model, dset, batch_size, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     21\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(), torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast():\n\u001b[0;32m---> 22\u001b[0m     image_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode_image(images)\n\u001b[1;32m     23\u001b[0m     image_features \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m image_features\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m     \u001b[39mif\u001b[39;00m all_img_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/open_clip/model.py:211\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image, normalize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, image, normalize: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 211\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual(image)\n\u001b[1;32m    212\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mnormalize(features, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m normalize \u001b[39melse\u001b[39;00m features\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/open_clip/transformer.py:460\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    457\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre(x)\n\u001b[1;32m    459\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    461\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_pool \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/open_clip/transformer.py:319\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    317\u001b[0m         x \u001b[39m=\u001b[39m checkpoint(r, x, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, attn_mask)\n\u001b[1;32m    318\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         x \u001b[39m=\u001b[39m r(x, attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n\u001b[1;32m    320\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/open_clip/transformer.py:243\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    240\u001b[0m v_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(v_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    242\u001b[0m x \u001b[39m=\u001b[39m q_x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(q_x\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(q_x), k_x\u001b[39m=\u001b[39mk_x, v_x\u001b[39m=\u001b[39mv_x, attn_mask\u001b[39m=\u001b[39mattn_mask))\n\u001b[0;32m--> 243\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(x)))\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/content_understanding/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_eces_uncalib = {}\n",
    "for dataset_name in ['CIFAR100', 'CIFAR10', 'Food101', 'SUN397']:\n",
    "    dset,_ = get_test_set(dataset_name, None)\n",
    "    model_eces = {}\n",
    "    for model_legend, temp in tqdm(all_temps.items()):\n",
    "        model_name, pretrained_dset = model_legend.split(';')\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(model_name,\n",
    "            pretrained=pretrained_dset,\n",
    "            device=device)\n",
    "        tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        if isinstance(dset, torch.utils.data.dataset.Subset):\n",
    "            dset.dataset.transform = preprocess\n",
    "        else:\n",
    "            dset.transform = preprocess\n",
    "        _ , templates = get_openai_prompts(dataset_name)\n",
    "\n",
    "        template_eces = []\n",
    "        image_features, actual = get_image_features(\n",
    "            model,  dset,  batch_size=batch_size, device=device\n",
    "        )\n",
    "        for text_template in templates:\n",
    "            #predictions, actual, probs = get_preds(model, tokenizer, dset, \n",
    "            #    text_template=text_template, temp_scaling=temp, device=device)\n",
    "\n",
    "            predictions, probs = get_preds_from_img_features(model, tokenizer, dset, image_features, text_template=text_template, \n",
    "                #temp_scaling=temp,\n",
    "                device = device)\n",
    "\n",
    "            ECE, _, acc = get_metrics(predictions, actual, probs)\n",
    "            template_eces.append(ECE)\n",
    "        \n",
    "        model_eces[model_legend] = template_eces\n",
    "    dataset_eces_uncalib[dataset_name] = model_eces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/13 [02:47<33:27, 167.30s/it]"
     ]
    }
   ],
   "source": [
    "for dataset_name in ['SUN397']:\n",
    "    dset,_ = get_test_set(dataset_name, None)\n",
    "    model_eces = {}\n",
    "    for model_legend, temp in tqdm(all_temps.items()):\n",
    "        model_name, pretrained_dset = model_legend.split(';')\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(model_name,\n",
    "            pretrained=pretrained_dset,\n",
    "            device=device)\n",
    "        tokenizer = open_clip.get_tokenizer(model_name)\n",
    "        if isinstance(dset, torch.utils.data.dataset.Subset):\n",
    "            dset.dataset.transform = preprocess\n",
    "        else:\n",
    "            dset.transform = preprocess\n",
    "        _ , templates = get_openai_prompts(dataset_name)\n",
    "\n",
    "        template_eces = []\n",
    "        image_features, actual = get_image_features(\n",
    "            model,  dset,  batch_size=batch_size, device=device\n",
    "        )\n",
    "        for text_template in templates:\n",
    "            #predictions, actual, probs = get_preds(model, tokenizer, dset, \n",
    "            #    text_template=text_template, temp_scaling=temp, device=device)\n",
    "\n",
    "            predictions, probs = get_preds_from_img_features(model, tokenizer, dset, image_features, text_template=text_template, \n",
    "                #temp_scaling=temp,\n",
    "                device = device)\n",
    "\n",
    "            ECE, _, acc = get_metrics(predictions, actual, probs)\n",
    "            template_eces.append(ECE)\n",
    "        \n",
    "        model_eces[model_legend] = template_eces\n",
    "    dataset_eces_uncalib[dataset_name] = model_eces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features, actual = get_image_features(model, imagenet_test, batch_size=128,\n",
    "        device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = torch.IntTensor(actual).to(device).long()\n",
    "actual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mapping = map_imagenet_to_readable_label()\n",
    "imagenet_test.classes = [imagenet_mapping[x] for x in imagenet_test.classes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer([text_template.replace('{}',x) for x in imagenet_test.classes])\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    text_features = model.encode_text(text.to(device))\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "text_probs = (100.0 * image_features @ text_features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6131701469421387"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Setup LBGFS\n",
    "temperature = nn.Parameter((torch.ones(1)).to(device))\n",
    "args = {'temperature': temperature}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Removing strong_wolfe line search results in jump after 50 epochs\n",
    "optimizer = optim.LBFGS([temperature], lr=0.001, max_iter=1000, line_search_fn='strong_wolfe')\n",
    "\n",
    "temps = []\n",
    "losses = []\n",
    "def _eval():\n",
    "    loss = criterion(T_scaling(text_probs, args), actual)\n",
    "    loss.backward()\n",
    "    temps.append(temperature.item())\n",
    "    losses.append(loss)\n",
    "    return loss\n",
    "optimizer.step(_eval)\n",
    "temperature.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.620787089606212, 0.958984375, 0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, probs = get_preds_from_img_features(\n",
    "    model, tokenizer, imagenet_test, image_features, text_template=text_template, temp_scaling=temperature.item(), device = device\n",
    ")\n",
    "get_metrics(predictions, actual, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.025181555526703616, 0.057354552355015076, 0.7139)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_test, num_classes = get_test_set('CIFAR100', preprocess)\n",
    "predictions, actual, probs = get_preds(model, tokenizer, cifar_test, \n",
    "    text_template=text_template, temp_scaling=temperature.item(), device=device)\n",
    "get_metrics(predictions, actual, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.007352347984910052, 0.18847142159938812, 0.9174)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_test, num_classes = get_test_set('CIFAR10', preprocess)\n",
    "predictions, actual, probs = get_preds(model, tokenizer, cifar_test, \n",
    "    text_template=text_template, temp_scaling=temperature.item(), device=device)\n",
    "get_metrics(predictions, actual, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content_understanding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d36b139402f0f8909133622e5e80cdd43397350f551386f6df555aa508ab69d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
